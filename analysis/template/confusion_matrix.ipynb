{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Antoine A. Ruzette\n",
    "Date: 2025-02-21\n",
    "\n",
    "This notebook processes cell measurement tables exported from QuPath to plot the spatial distribution of cell-level pixel intensity in relation to a modelled stromal border. It also supports the comparison of confusion matrices between threshold- and machine learning-based cell classification.\n",
    "\n",
    "Contains the code to plot data from images containing four channels: DAPI (nuclei), TRITC (cytokeratin), FITC (fibronectin) and CY5 (Ki67). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import natsort\n",
    "import re\n",
    "from matplotlib.ticker import LogFormatterSciNotation\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "# colorblind-friendly colors\n",
    "CB_palette = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                '#f781bf', '#a65628', '#984ea3',\n",
    "                '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-end functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_files(folder_path, file_paths, expected_columns):\n",
    "    \"\"\"\n",
    "    Load and preprocess CSV files.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        file_paths (list): List of file names.\n",
    "        expected_columns (dict): Dictionary mapping expected column keys to possible names.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed DataFrames (one per image).\n",
    "        dict: Column mapping for use in plotting.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    final_column_mapping = {}\n",
    "\n",
    "    for image in file_paths:\n",
    "        print(f\"\\nğŸ”¹ Processing: {image}\")\n",
    "        file_path = os.path.join(folder_path, image)\n",
    "\n",
    "        # Load CSV to check available columns first\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file_path, nrows=1)\n",
    "            available_columns = df_sample.columns.tolist()\n",
    "            print(f\"âœ… Available Columns: {available_columns}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading {image}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Dynamically map expected column names to available ones\n",
    "        column_mapping = {}\n",
    "        for key, possible_names in expected_columns.items():\n",
    "            for name in possible_names:\n",
    "                if name in available_columns:\n",
    "                    column_mapping[key] = name\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: {key} column not found in {image}. Skipping.\")\n",
    "\n",
    "        if not column_mapping:\n",
    "            print(f\"âš ï¸ Skipping {image} as no expected columns were found.\")\n",
    "            continue\n",
    "\n",
    "        # Reload dataframe with only found columns\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=list(column_mapping.values()))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading selected columns in {image}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Skip this file if essential columns are missing\n",
    "        essential_columns = [\"DAPI\", \"Ki67_647\"]\n",
    "        missing_essential = [col for col in essential_columns if col not in column_mapping]\n",
    "        if missing_essential:\n",
    "            print(f\"âš ï¸ Skipping {image} due to missing essential columns: {missing_essential}\")\n",
    "            continue\n",
    "\n",
    "        # Remove outliers dynamically (only for present columns)\n",
    "        outlier_limits = {}\n",
    "        for key in [\"DAPI\", \"Ki67_647\", \"KER_488\", \"FN_568\"]:\n",
    "            if key in column_mapping:\n",
    "                col_name = column_mapping[key]\n",
    "                p01 = df[col_name].quantile(0.01)\n",
    "                p99 = df[col_name].quantile(0.99)\n",
    "                outlier_limits[col_name] = (p01, p99)\n",
    "\n",
    "        print(f\"ğŸ“Š Outlier Thresholds: {outlier_limits}\")\n",
    "\n",
    "        # Filter outliers\n",
    "        df_no_outlier = df.copy()\n",
    "        for col, (p01, p99) in outlier_limits.items():\n",
    "            df_no_outlier = df_no_outlier[(df_no_outlier[col] >= p01) & (df_no_outlier[col] <= p99)]\n",
    "\n",
    "        if df_no_outlier.empty:\n",
    "            print(f\"âš ï¸ Skipping {image} as it became empty after outlier removal.\")\n",
    "            continue\n",
    "\n",
    "        # âœ… Store actual filename instead of \"1G\", \"2G\" etc.\n",
    "        df_no_outlier[\"Image\"] = os.path.splitext(os.path.basename(image))[0]\n",
    "\n",
    "        dfs.append(df_no_outlier)\n",
    "\n",
    "        # Save column mapping for later use\n",
    "        final_column_mapping = column_mapping  \n",
    "\n",
    "    if dfs:\n",
    "        return dfs, final_column_mapping\n",
    "    else:\n",
    "        print(\"âš ï¸ No valid data loaded.\")\n",
    "        return [], {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix to compare ML vs threshold-based classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if datasets have same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = {\n",
    "    \"Object ID\": [\"Object ID\"],\n",
    "    \"Class\": [\"Classification\"],\n",
    "    \"DAPI\": [\"DAPI: Nucleus: Median\"],\n",
    "    \"KER_488\": [\"FITC KER: Cytoplasm: Median\"],\n",
    "    \"Ki67_647\": [\"CY5 Ki67: Nucleus: Max\"],\n",
    "    \"FN_568\": [\"TRITC FN: Cell: Median\"],\n",
    "    \"Nucleus_Area\": [\"Nucleus: Area Âµm^2\"]\n",
    "}\n",
    "\n",
    "folder_thr = \"path/to/your/thresholds/folder\"\n",
    "folder_ML = \"path/to/your/ML/folder\"\n",
    "\n",
    "file_paths = [\n",
    "    \"all_measurements.csv\"\n",
    "]\n",
    "\n",
    "thr_label, _ = load_and_preprocess_files(folder_thr, file_paths, expected_columns)\n",
    "ML_all, _ = load_and_preprocess_files(folder_ML, file_paths, expected_columns)\n",
    "\n",
    "\n",
    "# Now safely perform set operations on 'Object ID'\n",
    "common_object_ids = len(set(ML_all[0]['Object ID']).intersection(set(thr_label[0]['Object ID'])))\n",
    "print(\"Number of common Object IDs between the two datasets:\", common_object_ids)\n",
    "\n",
    "flag = (len(set(ML_all[0]['Object ID'])) == len(set(thr_label[0]['Object ID'])))\n",
    "print(\"Datasets have the same number of cells:\", flag)\n",
    "\n",
    "# Create a lookup table for class name conversion\n",
    "class_lookup = {\n",
    "    'FITC KER: CY5 Ki67': 'Ki67+: KER+',\n",
    "    'FITC KER': 'Ki67-: KER+',\n",
    "    'CY5 Ki67': 'Ki67+: KER-',\n",
    "    # Add more mappings as needed\n",
    "    np.nan: 'Ki67-: KER-'  # Represent NaN as a class itself\n",
    "}\n",
    "\n",
    "\n",
    "# Apply the lookup table to both datasets\n",
    "ML_all[0]['Classification'] = ML_all[0]['Classification'].map(class_lookup)\n",
    "\n",
    "# # Print class distributions to verify\n",
    "print(\"Updated Thresholds dataset classifications:\")\n",
    "print(thr_label[0]['Classification'].value_counts())\n",
    "\n",
    "print(\"\\nUpdated ML dataset classifications:\")\n",
    "print(ML_all[0]['Classification'].value_counts())\n",
    "\n",
    "thr_label = thr_label[0]\n",
    "ML_all = ML_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Thresholds dataset classifications:\")\n",
    "print(thr_label['Classification'].value_counts())\n",
    "\n",
    "print(\"\\nML dataset classifications:\")\n",
    "print(ML_all['Classification'].value_counts())\n",
    "\n",
    "# Merge the datasets on the 'Object ID' column\n",
    "merged_data = pd.merge(ML_all, thr_label, on='Object ID', suffixes=('_ML', '_Thr'))\n",
    "\n",
    "# Get common 'Object ID' cells between the two datasets\n",
    "common_object_ids = set(ML_all['Object ID']).intersection(set(thr_label['Object ID']))\n",
    "\n",
    "# Filter the merged data to include only common 'Object ID' cells\n",
    "merged_data_common = merged_data[merged_data['Object ID'].isin(common_object_ids)]\n",
    "\n",
    "# Initialize counts for each class combination\n",
    "class_counts = {\n",
    "    'Ki67+: KER+': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0, 'Ki67+: KER-': 0, 'Ki67-: KER-': 0},\n",
    "    'Ki67-: KER+': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0, 'Ki67+: KER-': 0, 'Ki67-: KER-': 0},\n",
    "    'Ki67+: KER-': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0, 'Ki67+: KER-': 0, 'Ki67-: KER-': 0},\n",
    "    'Ki67-: KER-': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0, 'Ki67+: KER-': 0, 'Ki67-: KER-': 0}\n",
    "}\n",
    "\n",
    "# Iterate through the merged data and count occurrences of each class combination\n",
    "for _, row in merged_data_common.iterrows():\n",
    "    class_counts[row['Classification_ML']][row['Classification_Thr']] += 1\n",
    "\n",
    "# Print the class counts\n",
    "for true_class, predicted_classes in class_counts.items():\n",
    "    print(f'True Class: {true_class}')\n",
    "    for predicted_class, count in predicted_classes.items():\n",
    "        print(f'Predicted Class: {predicted_class}, Count: {count}')\n",
    "    print()\n",
    "\n",
    "# Define class labels\n",
    "classes = ['Ki67+: KER+', 'Ki67-: KER+', 'Ki67+: KER-', 'Ki67-: KER-']\n",
    "#classes = ['KER+: Ki67+', 'KER+: Ki67-', 'KER-: Ki67+', 'KER-: Ki67-']\n",
    "\n",
    "# Create a DataFrame from class counts dictionary\n",
    "confusion_df = pd.DataFrame(class_counts, index=classes)\n",
    "\n",
    "# Sum up counts along the diagonal (correct predictions)\n",
    "correct_predictions = np.diag(confusion_df)\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = confusion_df.values.sum()\n",
    "\n",
    "# Calculate ratio of agreement\n",
    "agreement_ratio = (correct_predictions.sum() / total_samples) * 100\n",
    "\n",
    "print(\"Ratio of agreement between the two methods:\", agreement_ratio)\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 30})\n",
    "plt.title(f'Confusion matrix (accuracy: {agreement_ratio:.1f}%)', fontsize=25)\n",
    "# plt.ylabel('Threshold-based classification', fontsize=18)\n",
    "# plt.xlabel('ML-based classification', fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "# Set scientific notation for the colorbar\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "cbar.ax.tick_params(labelsize=18)  # Set colorbar tick labels font size\n",
    "\n",
    "# Increase font size of the exponent (scientific notation) above the colorbar\n",
    "if cbar.ax.yaxis.get_offset_text():  # Check if offset text exists\n",
    "    cbar.ax.yaxis.get_offset_text().set_size(18)  # Set its font size\n",
    "    \n",
    "plt.savefig(f\"{folder_thr}/confusion_matrix_full.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict to only two classes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_thr_subset = thr_label[thr_label['Classification'].isin(['Ki67+: KER+', 'Ki67-: KER+'])]\n",
    "all_ML_subset = ML_all[ML_all['Classification'].isin(['Ki67+: KER+', 'Ki67-: KER+'])]\n",
    "\n",
    "# Merge the datasets on the 'Object ID' column\n",
    "merged_data = pd.merge(all_ML_subset, all_thr_subset, on='Object ID', suffixes=('_ML', '_Thr'))\n",
    "\n",
    "# Get common 'Object ID' cells between the two datasets\n",
    "common_object_ids = set(all_ML_subset['Object ID']).intersection(set(all_thr_subset['Object ID']))\n",
    "\n",
    "# Filter the merged data to include only common 'Object ID' cells\n",
    "merged_data_common = merged_data[merged_data['Object ID'].isin(common_object_ids)]\n",
    "\n",
    "# Initialize counts for each class combination\n",
    "class_counts = {\n",
    "    'Ki67+: KER+': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0},\n",
    "    'Ki67-: KER+': {'Ki67+: KER+': 0, 'Ki67-: KER+': 0},\n",
    "}\n",
    "\n",
    "# Iterate through the merged data and count occurrences of each class combination\n",
    "for _, row in merged_data_common.iterrows():\n",
    "    class_counts[row['Classification_ML']][row['Classification_Thr']] += 1\n",
    "\n",
    "# Print the class counts\n",
    "for true_class, predicted_classes in class_counts.items():\n",
    "    print(f'True Class: {true_class}')\n",
    "    for predicted_class, count in predicted_classes.items():\n",
    "        print(f'Predicted Class: {predicted_class}, Count: {count}')\n",
    "    print()\n",
    "\n",
    "# Define class labels\n",
    "classes = ['Ki67+: KER+', 'Ki67-: KER+']\n",
    "#classes = ['KER+: Ki67+', 'KER+: Ki67-', 'KER-: Ki67+', 'KER-: Ki67-']\n",
    "\n",
    "# Create a DataFrame from class counts dictionary\n",
    "confusion_df = pd.DataFrame(class_counts, index=classes)\n",
    "\n",
    "# Sum up counts along the diagonal (correct predictions)\n",
    "correct_predictions = np.diag(confusion_df)\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = confusion_df.values.sum()\n",
    "\n",
    "# Calculate ratio of agreement\n",
    "agreement_ratio = (correct_predictions.sum() / total_samples) * 100\n",
    "\n",
    "print(\"Ratio of agreement between the two methods:\", agreement_ratio)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(confusion_df, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 30})\n",
    "# plt.title(f'Confusion matrix (accuracy: {agreement_ratio:.1f}%)', fontsize=13)\n",
    "# plt.ylabel('Threshold-based classification', fontsize=18)\n",
    "# plt.xlabel('ML-based classification', fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=18)\n",
    "# Set scientific notation for the colorbar\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0))\n",
    "cbar.ax.tick_params(labelsize=18)  # Set colorbar tick labels font size\n",
    "\n",
    "# Increase font size of the exponent (scientific notation) above the colorbar\n",
    "if cbar.ax.yaxis.get_offset_text():  # Check if offset text exists\n",
    "    cbar.ax.yaxis.get_offset_text().set_size(18)  # Set its font size\n",
    "\n",
    "plt.savefig(f\"{folder_thr}/confusion_matrix_restricted.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
