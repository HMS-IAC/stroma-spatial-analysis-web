{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Antoine A. Ruzette\n",
    "Date: 2025-02-21\n",
    "\n",
    "This notebook processes cell measurement tables exported from QuPath to plot the spatial distribution of cell-level pixel intensity in relation to a modelled stromal border. It also supports the comparison of confusion matrices between threshold- and machine learning-based cell classification.\n",
    "\n",
    "Contains the code to plot data from images containing four channels: DAPI (nuclei), TRITC (cytokeratin), FITC (fibronectin) and CY5 (Ki67). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"fitter>=1.6.0\" \"ipykernel>=6.29.5\" \"matplotlib>=3.10.0\" \"natsort>=8.4.0\" \"numpy>=2.2.3\" \"pandas>=2.2.3\" \"scipy>=1.15.2\" \"seaborn>=0.13.2\" \"setuptools>=75.8.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fitter import Fitter\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "# colorblind-friendly colors\n",
    "CB_palette = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                '#f781bf', '#a65628', '#984ea3',\n",
    "                '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-end functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_files(folder_path, file_paths, expected_columns):\n",
    "    \"\"\"\n",
    "    Load and preprocess CSV files.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        file_paths (list): List of file names.\n",
    "        expected_columns (dict): Dictionary mapping expected column keys to possible names.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed DataFrames (one per image).\n",
    "        dict: Column mapping for use in plotting.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    final_column_mapping = {}\n",
    "\n",
    "    for image in file_paths:\n",
    "        print(f\"\\nðŸ”¹ Processing: {image}\")\n",
    "        file_path = os.path.join(folder_path, image)\n",
    "\n",
    "        # Load CSV to check available columns first\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file_path, nrows=1)\n",
    "            available_columns = df_sample.columns.tolist()\n",
    "            print(f\"âœ… Available Columns: {available_columns}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading {image}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Dynamically map expected column names to available ones\n",
    "        column_mapping = {}\n",
    "        for key, possible_names in expected_columns.items():\n",
    "            for name in possible_names:\n",
    "                if name in available_columns:\n",
    "                    column_mapping[key] = name\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: {key} column not found in {image}. Skipping.\")\n",
    "\n",
    "        if not column_mapping:\n",
    "            print(f\"âš ï¸ Skipping {image} as no expected columns were found.\")\n",
    "            continue\n",
    "\n",
    "        # Reload dataframe with only found columns\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, usecols=list(column_mapping.values()))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading selected columns in {image}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Skip this file if essential columns are missing\n",
    "        essential_columns = [\"DAPI\", \"Ki67_647\"]\n",
    "        missing_essential = [col for col in essential_columns if col not in column_mapping]\n",
    "        if missing_essential:\n",
    "            print(f\"âš ï¸ Skipping {image} due to missing essential columns: {missing_essential}\")\n",
    "            continue\n",
    "\n",
    "        # Remove outliers dynamically (only for present columns)\n",
    "        outlier_limits = {}\n",
    "        for key in [\"DAPI\", \"Ki67_647\", \"KER_488\", \"FN_568\"]:\n",
    "            if key in column_mapping:\n",
    "                col_name = column_mapping[key]\n",
    "                p01 = df[col_name].quantile(0.01)\n",
    "                p99 = df[col_name].quantile(0.99)\n",
    "                outlier_limits[col_name] = (p01, p99)\n",
    "\n",
    "        print(f\"ðŸ“Š Outlier Thresholds: {outlier_limits}\")\n",
    "\n",
    "        # Filter outliers\n",
    "        df_no_outlier = df.copy()\n",
    "        for col, (p01, p99) in outlier_limits.items():\n",
    "            df_no_outlier = df_no_outlier[(df_no_outlier[col] >= p01) & (df_no_outlier[col] <= p99)]\n",
    "\n",
    "        if df_no_outlier.empty:\n",
    "            print(f\"âš ï¸ Skipping {image} as it became empty after outlier removal.\")\n",
    "            continue\n",
    "\n",
    "        # âœ… Store actual filename instead of \"1G\", \"2G\" etc.\n",
    "        df_no_outlier[\"Image\"] = os.path.splitext(os.path.basename(image))[0]\n",
    "\n",
    "        dfs.append(df_no_outlier)\n",
    "\n",
    "        # Save column mapping for later use\n",
    "        final_column_mapping = column_mapping  \n",
    "\n",
    "    if dfs:\n",
    "        return dfs, final_column_mapping\n",
    "    else:\n",
    "        print(\"âš ï¸ No valid data loaded.\")\n",
    "        return [], {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring non-gaussian distributions using the `Fitter` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.g., ki67 histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best fit distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following to use your data\n",
    "########################################################\n",
    "\n",
    "# Define folder and file paths\n",
    "folder_path = \"path/to/your/folder\"\n",
    "\n",
    "file_paths = [\n",
    "    \"data1.csv\",\n",
    "    \"data2.csv\",\n",
    "]\n",
    "\n",
    "# Expected column mappings\n",
    "expected_columns = {\n",
    "    \"Class\": [\"Class\"],\n",
    "    \"DAPI\": [\"DAPI: Nucleus: Median\"],\n",
    "    \"KER_488\": [\"FITC KER: Cytoplasm: Median\"],\n",
    "    \"Ki67_647\": [\"CY5 Ki67: Nucleus: Max\"],\n",
    "    \"FN_568\": [\"TRITC FN: Cell: Median\"],\n",
    "    \"Nucleus_Area\": [\"Nucleus: Area Âµm^2\"]\n",
    "}\n",
    "\n",
    "# Define bin sizes for each channel\n",
    "bin_sizes = {\"DAPI\": 150, \"Ki67_647\": 5, \"KER_488\": 50, \"FN_568\": 20}\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Define folder and file paths\n",
    "folder_path = \"path/to/your/folder\"\n",
    "\n",
    "file_paths = [\n",
    "    \"data1.csv\",\n",
    "    \"data2.csv\",\n",
    "]\n",
    "\n",
    "# Define bin sizes for each channel\n",
    "bin_sizes = {\n",
    "    \"DAPI\": 50,\n",
    "    \"Ki67_647\": 5,\n",
    "    \"KER_488\": 75,\n",
    "    \"FN_568\": 40\n",
    "}\n",
    "\n",
    "expected_columns = {\n",
    "    \"Class\": [\"Class\"],\n",
    "    \"DAPI\": [\"DAPI: Nucleus: Median\"],\n",
    "    \"KER_488\": [\"FITC KER: Cytoplasm: Median\"],\n",
    "    \"Ki67_647\": [\"CY5 Ki67: Nucleus: Max\"],\n",
    "    \"FN_568\": [\"TRITC FN: Cell: Median\"],\n",
    "    \"Nucleus_Area\": [\"Nucleus: Area Âµm^2\"]\n",
    "}\n",
    "\n",
    "# Define axis labels and limits\n",
    "axis_labels = {\n",
    "    \"PDF\": \"Ki67 max\\nnuclear intensity, a.u.\",\n",
    "    \"CDF\": \"Ki67 max\\nnuclear intensity, a.u.\"\n",
    "}\n",
    "axis_limits = {\n",
    "    \"PDF\": (0, 3000),\n",
    "    \"CDF\": (0, 3000)\n",
    "}\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "dfs, column_mapping = load_and_preprocess_files(folder_path, file_paths, expected_columns)\n",
    "\n",
    "# Initialize plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(34, 16))\n",
    "\n",
    "# Define bin width for histogram and fitting\n",
    "bin_width = bin_sizes.get(\"Ki67_647\", 5)\n",
    "\n",
    "for idx, df in enumerate(dfs):\n",
    "    image_name = df[\"Image\"].iloc[0]\n",
    "    gem_number = f\"#{idx + 1}\"\n",
    "\n",
    "    # Get relevant column name\n",
    "    pNDRG1_col = column_mapping.get(\"Ki67_647\", \"Ki67_647: Nucleus: Max\")\n",
    "\n",
    "    # Define bin edges based on min and max values\n",
    "    min_val = df[pNDRG1_col].min()\n",
    "    max_val = df[pNDRG1_col].max()\n",
    "    bins = np.arange(min_val, max_val + bin_width, bin_width)\n",
    "\n",
    "    # Fit log-normal distribution\n",
    "    f = Fitter(df[pNDRG1_col], distributions=[\"lognorm\"])\n",
    "    f.fit()\n",
    "    params = f.get_best(method=\"sumsquare_error\").get(\"lognorm\", {})\n",
    "\n",
    "    # Compute fitted PDF and CDF at the same resolution as histogram\n",
    "    pdf_values = lognorm.pdf(bins, *params.values())\n",
    "    cdf_values = lognorm.cdf(bins, *params.values())\n",
    "    pdf_values *= bin_width\n",
    "\n",
    "    # Overlay histogram and best-fit PDF\n",
    "    sns.histplot(df[pNDRG1_col], \n",
    "                 bins=bins, \n",
    "                 kde=False,\n",
    "                 ax=axs[0], \n",
    "                 label=f'{gem_number}',\n",
    "                 alpha=0.5,\n",
    "                 stat='probability')\n",
    "    axs[0].plot(bins, pdf_values, label=f\"PDF {gem_number}\", linewidth=7)\n",
    "\n",
    "    # Plot CDF separately\n",
    "    axs[1].plot(bins, cdf_values, label=f'CDF {gem_number}', linewidth=7)\n",
    "    axs[1].set_ylim(0, 1)\n",
    "# Apply formatting to match your layout\n",
    "for key, ax in zip([\"PDF\", \"CDF\"], axs):\n",
    "    ax.set_yscale('linear')\n",
    "    ax.set_xscale('linear')\n",
    "    ax.set_ylabel('Probability', fontsize=70)\n",
    "    ax.set_xlabel(axis_labels[key], fontsize=70)\n",
    "    ax.tick_params(axis='both', labelsize=55)\n",
    "\n",
    "    # Set x-axis range and define 5 evenly spaced ticks\n",
    "    if key in axis_limits:\n",
    "        ax.set_xlim(axis_limits[key])\n",
    "        xticks = np.linspace(axis_limits[key][0], axis_limits[key][1], 5)\n",
    "        ax.set_xticks(xticks)\n",
    "\n",
    "    # Ensure y-axis tick limits apply to all panels\n",
    "    yticks = ax.get_yticks()\n",
    "    if len(yticks) > 5:  \n",
    "        min_y, max_y = min(yticks), max(yticks)  \n",
    "        yticks = np.linspace(min_y, max_y, 5)  \n",
    "        ax.set_yticks(yticks)\n",
    "\n",
    "    # Format y-tick labels to 2 significant digits\n",
    "    ax.set_yticklabels([f\"{tick:.2g}\" for tick in yticks])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{folder_path}/best_fit_distribution.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the best fit distribution to translate a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the following to use your data\n",
    "########################################################\n",
    "\n",
    "# Define folder and file paths\n",
    "folder_path = \"path/to/your/folder\"\n",
    "\n",
    "file_paths = [\n",
    "    \"data1.csv\",\n",
    "    \"data2.csv\",\n",
    "]\n",
    "\n",
    "# Expected column mappings\n",
    "expected_columns = {\n",
    "    \"Class\": [\"Class\"],\n",
    "    \"DAPI\": [\"DAPI: Nucleus: Median\"],\n",
    "    \"KER_488\": [\"FITC KER: Cytoplasm: Median\"],\n",
    "    \"Ki67_647\": [\"CY5 Ki67: Nucleus: Max\"],\n",
    "    \"FN_568\": [\"TRITC FN: Cell: Median\"],\n",
    "    \"Nucleus_Area\": [\"Nucleus: Area Âµm^2\"]\n",
    "}\n",
    "\n",
    "# Define bin sizes for each channel\n",
    "bin_sizes = {\"DAPI\": 150, \"Ki67_647\": 5, \"KER_488\": 50, \"FN_568\": 20}\n",
    "\n",
    "########################################################\n",
    "\n",
    "# Load and preprocess data\n",
    "dfs, column_mapping = load_and_preprocess_files(folder_path, file_paths, expected_columns)\n",
    "\n",
    "# Initialize plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(34, 15))\n",
    "\n",
    "# Define bin width for histogram and fitting\n",
    "bin_width = bin_sizes.get(\"Ki67_647\", 5)\n",
    "\n",
    "# Placeholder for storing reference distribution parameters\n",
    "params_reference = None\n",
    "threshold_reference = 950  # Example threshold value to translate\n",
    "\n",
    "for idx, df in enumerate(dfs):\n",
    "    image_name = df[\"Image\"].iloc[0]\n",
    "    gem_number = f\"#{idx + 1}\"\n",
    "    # image_color = image_colors.get(image_name, \"gray\")\n",
    "    pNDRG1_col = column_mapping.get(\"Ki67_647\", \"Ki67_647: Nuclear: Max\")\n",
    "\n",
    "    # Define bin edges based on min and max values\n",
    "    min_val = df[pNDRG1_col].min()\n",
    "    max_val = df[pNDRG1_col].max()\n",
    "    bins = np.arange(min_val, max_val + bin_width, bin_width)\n",
    "\n",
    "    # Fit log-normal distribution\n",
    "    f = Fitter(df[pNDRG1_col], distributions=[\"lognorm\"])\n",
    "    f.fit()\n",
    "    params = f.get_best(method=\"sumsquare_error\").get(\"lognorm\", {})\n",
    "\n",
    "    if idx == 0:\n",
    "        params_reference = params\n",
    "    else:\n",
    "        cdf_reference = lognorm.cdf(threshold_reference, **params_reference)\n",
    "        translated_threshold = lognorm.ppf(cdf_reference, **params)\n",
    "        scaled_threshold = (threshold_reference - params_reference['loc']) * (params['scale'] / params_reference['scale']) + params['loc']\n",
    "        print(f\"Translated threshold for img{idx}: {image_name}; {translated_threshold:.2f}\")\n",
    "        print(f\"Scaled threshold for img{idx}: {scaled_threshold:.2f}\")\n",
    "        print(\"params_reference\", params_reference)\n",
    "        print(\"params\", params)\n",
    "        \n",
    "    # Plot histograms and fitted distributions\n",
    "    sns.histplot(df[pNDRG1_col], bins=bins, kde=False, ax=axs[0], alpha=0.5, stat='probability', label=gem_number)\n",
    "    pdf_values = lognorm.pdf(bins, *params.values()) * bin_width\n",
    "    cdf_values = lognorm.cdf(bins, *params.values())\n",
    "    axs[0].plot(bins, pdf_values, label=f\"PDF {gem_number}\", linewidth=7)\n",
    "    axs[1].plot(bins, cdf_values, label=f\"CDF {gem_number}\", linewidth=7)\n",
    "    axs[1].set_ylim(0, 1)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.legend(fontsize=20)\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.set_xlim(0, 8000)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(folder_path, \"fitted_measurements_distribution_pNDRG1.png\"), dpi=300)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
